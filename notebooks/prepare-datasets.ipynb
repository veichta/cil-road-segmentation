{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"../data\"\n",
    "datasets = [\"CIL-dataset\"]\n",
    "datasets_out = [\"DeepGlobe\", \"MRD\", \"CIL\"]\n",
    "dataset_out = \"../data/joint-dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(dataset_out):\n",
    "    os.mkdir(dataset_out)\n",
    "    for ds in datasets_out:\n",
    "        os.mkdir(os.path.join(dataset_out, ds))\n",
    "        os.mkdir(os.path.join(dataset_out, ds, \"images\"))\n",
    "        os.mkdir(os.path.join(dataset_out, ds, \"groundtruth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open(os.path.join(data, datasets[0], \"train\", \"104_mask.png\"))\n",
    "im.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get patches of size (400, 400) from the image\n",
    "def get_patches(im, size):  # sourcery skip: for-append-to-extend\n",
    "    patches = []\n",
    "    for i in range(0, im.size[0] - size[0], size[0]//2):\n",
    "        for j in range(0, im.size[1] - size[1], size[1]//2):\n",
    "            patches.append(im.crop((i, j, i + size[0], j + size[1])))\n",
    "    return patches\n",
    "\n",
    "patches = get_patches(im, (400, 400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Patches from DeepGlobe\n",
    "This Dataset can be downloaded here: https://www.kaggle.com/datasets/balraj98/deepglobe-road-extraction-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets[0]\n",
    "\n",
    "# get unique names\n",
    "fnames = os.listdir(os.path.join(data, dataset, \"train\"))\n",
    "fnames = list({name.split(\"_\")[0] for name in fnames})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in tqdm(fnames):\n",
    "    img = Image.open(os.path.join(data, dataset, \"train\", f\"{fname}_sat.jpg\"))\n",
    "    target = Image.open(os.path.join(data, dataset, \"train\", f\"{fname}_mask.png\"))\n",
    "\n",
    "    # get patches of size (400, 400) from the image\n",
    "    img_patches = get_patches(img, (400, 400))\n",
    "    target_patches = get_patches(target, (400, 400))\n",
    "\n",
    "    # save patches to disk\n",
    "    idx = uuid.uuid4()\n",
    "    for i in range(len(img_patches)):\n",
    "        if np.sum(target_patches[i]) > 0:\n",
    "            img_patches[i].save(os.path.join(dataset_out, \"DeepGlobe\", \"images\", f\"{idx}-{i}.jpg\"))\n",
    "            target_patches[i].save(os.path.join(dataset_out, \"DeepGlobe\", \"groundtruth\", f\"{idx}-{i}-mask.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Patches from MRD\n",
    "This dataset can be downloaded here: https://www.kaggle.com/datasets/balraj98/massachusetts-roads-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = f\"{datasets[1]}/tiff\"\n",
    "\n",
    "for split in [\"train\", \"test\", \"val\"]:\n",
    "    # get unique names\n",
    "    fnames = os.listdir(os.path.join(data, dataset, split))\n",
    "    \n",
    "    for fname in tqdm(fnames):\n",
    "        # load .tiff image\n",
    "        img = Image.open(os.path.join(data, dataset, split, fname))\n",
    "\n",
    "        # img = Image.open(os.path.join(data, dataset, \"train\", f\"{fname}\"))\n",
    "        target = Image.open(os.path.join(data, dataset, f\"{split}_labels\", f\"{fname}\"[:-1]))\n",
    "\n",
    "        # get patches of size (400, 400) from the image\n",
    "        img_patches = get_patches(img, (400, 400))\n",
    "        target_patches = get_patches(target, (400, 400))\n",
    "\n",
    "        idx = uuid.uuid4()\n",
    "        for i in range(len(img_patches)):\n",
    "            keep = np.array(img_patches[i]).mean(axis=-1) > 254\n",
    "            s = np.sum(keep)\n",
    "            if s < 100 and np.sum(target_patches[i]) > 0:\n",
    "                img_patches[i].save(os.path.join(dataset_out, \"MRD\", \"images\", f\"{idx}-{i}.jpg\"))\n",
    "                target_patches[i].save(os.path.join(dataset_out, \"MRD\", \"groundtruth\", f\"{idx}-{i}-mask.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare CIL Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets[0]\n",
    "load_data = os.path.join(data, dataset, \"training\")\n",
    "\n",
    "for fname in tqdm(os.listdir(os.path.join(load_data, \"images\"))):\n",
    "    if fname.endswith(\".png\"):\n",
    "        img = Image.open(os.path.join(load_data, \"images\", fname))\n",
    "        target = Image.open(os.path.join(load_data, \"groundtruth\", fname))\n",
    "\n",
    "        # convert image to jpg\n",
    "        img = img.convert(\"RGB\")\n",
    "        target = target.convert(\"RGB\")\n",
    "        \n",
    "        img.save(os.path.join(dataset_out, \"CIL\", \"images\", f\"{fname.split('.')[0]}.jpg\"))\n",
    "        target.save(os.path.join(dataset_out, \"CIL\", \"groundtruth\", f\"{fname.split('.')[0]}-mask.png\"))\n",
    "\n",
    "load_data = os.path.join(data, dataset, \"test\")\n",
    "for fname in tqdm(os.listdir(os.path.join(load_data, \"images\"))):\n",
    "    if fname.endswith(\".png\"):\n",
    "        img = Image.open(os.path.join(load_data, \"images\", fname))\n",
    "        # convert image to jpg\n",
    "        img = img.convert(\"RGB\")\n",
    "        \n",
    "        img.save(os.path.join(dataset_out, \"CIL\", \"test\", f\"{fname.split('.')[0]}.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset_out\n",
    "df = []\n",
    "\n",
    "for dataset in [\"MRD\", \"DeepGlobe\", \"CIL\"]:\n",
    "    masks = os.listdir(os.path.join(data, dataset, \"groundtruth\"))\n",
    "    for i, mask in tqdm(enumerate(masks), total=len(masks)):\n",
    "        if mask.endswith(\"-mask.png\"):\n",
    "            n_pixels = np.sum(np.array(Image.open(os.path.join(data, dataset, \"groundtruth\", mask))) > 0)\n",
    "            df.append((mask, dataset, n_pixels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(df, columns=[\"filename\", \"dataset\", \"n_pixels\"])\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot hist per dataset of n_pixels in different plots using plt\n",
    "plt.figure(figsize=(10, 15))\n",
    "for i, dataset in enumerate([\"MRD\", \"DeepGlobe\", \"CIL\"]):\n",
    "    plt.subplot(3, 1, i + 1)\n",
    "    plt.hist(dataframe[dataframe[\"dataset\"] == dataset][\"n_pixels\"], bins=100, range=(0, 200000))\n",
    "    plt.title(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many masks have more than count pixels\n",
    "count = 10000\n",
    "dataframe[dataframe[\"n_pixels\"] > count].groupby(\"dataset\").count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create overview csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe containing all paths to the images\n",
    "data = dataset_out\n",
    "df = []\n",
    "\n",
    "for dataset in [\"CIL\"]:\n",
    "    images = os.listdir(os.path.join(data, dataset, \"images\"))\n",
    "    for i, img in tqdm(enumerate(images), total=len(images)):\n",
    "        fname = img.split(\".\")[0]\n",
    "        fpath = os.path.join(dataset, \"images\", img)\n",
    "        mask_path = os.path.join(dataset, \"groundtruth\", f\"{fname}-mask.png\")\n",
    "        df.append((fname, dataset, fpath, mask_path, \"train\"))\n",
    "\n",
    "for dataset in [\"CIL\"]:\n",
    "    images = sorted(os.listdir(os.path.join(data, dataset, \"test\")))\n",
    "    for i, img in tqdm(enumerate(images), total=len(images)):\n",
    "        fname = img.split(\".\")[0]\n",
    "        fpath = os.path.join(dataset, \"test\", img)\n",
    "        df.append((fname, dataset, fpath, \"\", \"test\"))\n",
    "        \n",
    "\n",
    "df = pd.DataFrame(df, columns=[\"filename\", \"dataset\", \"fpath\", 'mpath', \"split\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(dataset_out, \"dataset.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
